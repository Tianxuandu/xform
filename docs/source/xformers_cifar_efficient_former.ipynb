{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small example, on how to implement an EfficientFormer and train it on Cifar10. Please note that we pull in some code from xformers/examples here, so not all the training code is readily visible, but it's not very far away.\n",
    "\n",
    "Let's start with the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (1.12.0.dev20220417)\n",
      "Requirement already satisfied: typing_extensions in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from torch) (4.1.1)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: xformers in /home/lefaudeux/Git/xformers (0.0.11)\n",
      "Requirement already satisfied: pytorch_lightning in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (1.5.10)\n",
      "Requirement already satisfied: numpy in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (1.22.3)\n",
      "Requirement already satisfied: lightning-bolts in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (0.5.0)\n",
      "Requirement already satisfied: torchmetrics in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (0.7.3)\n",
      "Requirement already satisfied: torch>=1.8.1 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from xformers) (1.12.0.dev20220417)\n",
      "Requirement already satisfied: pyre-extensions==0.0.23 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from xformers) (0.0.23)\n",
      "Requirement already satisfied: typing-inspect in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from pyre-extensions==0.0.23->xformers) (0.7.1)\n",
      "Requirement already satisfied: typing-extensions in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from pyre-extensions==0.0.23->xformers) (4.1.1)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from pytorch_lightning) (6.0)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from pytorch_lightning) (2.8.0)\n",
      "Requirement already satisfied: future>=0.17.1 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from pytorch_lightning) (0.18.2)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from pytorch_lightning) (4.59.0)\n",
      "Requirement already satisfied: setuptools==59.5.0 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from pytorch_lightning) (59.5.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from pytorch_lightning) (21.3)\n",
      "Requirement already satisfied: pyDeprecate==0.3.1 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from pytorch_lightning) (0.3.1)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from pytorch_lightning) (2022.2.0)\n",
      "Requirement already satisfied: requests in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.27.1)\n",
      "Requirement already satisfied: aiohttp in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from packaging>=17.0->pytorch_lightning) (3.0.7)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.19.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.6.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.0.3)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.45.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.1)\n",
      "Requirement already satisfied: six in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch_lightning) (1.16.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (5.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.7.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (6.0.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages (from typing-inspect->pyre-extensions==0.0.23->xformers) (0.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --pre torch\n",
    "!pip install xformers pytorch_lightning numpy pytorch_lightning lightning-bolts torchmetrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import everything we need, and check that the above worked fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pytorch pre-release version 1.12.0.dev20220417 - assuming intent to test it\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pl_bolts.datamodules import CIFAR10DataModule\n",
    "from torch import nn\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "from examples.cifarViT import Classifier, VisionTransformer\n",
    "from xformers.components import MultiHeadDispatch\n",
    "from xformers.components.attention import ScaledDotProduct\n",
    "from xformers.components.patch_embedding import PatchEmbeddingConfig  # noqa\n",
    "from xformers.components.patch_embedding import build_patch_embedding  # noqa\n",
    "from xformers.factory import xFormer, xFormerConfig\n",
    "from xformers.helpers.hierarchical_configs import (\n",
    "    BasicLayerConfig,\n",
    "    get_hierarchical_configuration,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All good ! Now let's write down the EfficientFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EfficientFormer(VisionTransformer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        steps,\n",
    "        learning_rate=1e-2,\n",
    "        betas=(0.9, 0.99),\n",
    "        weight_decay=0.03,\n",
    "        image_size=32,\n",
    "        num_classes=10,\n",
    "        dim=384,\n",
    "        linear_warmup_ratio=0.1,\n",
    "        classifier=Classifier.GAP,\n",
    "    ):\n",
    "\n",
    "        super(VisionTransformer, self).__init__()\n",
    "\n",
    "        # all the inputs are saved under self.hparams (hyperparams)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Generate the skeleton of our hierarchical Transformer\n",
    "        # This implements a model close to the L1 suggested in \"EfficientFormer\" (https://arxiv.org/abs/2206.01191)\n",
    "        # but a pooling layer has been removed, due to the very small image dimensions in Cifar (32x32)\n",
    "        base_hierarchical_configs = [\n",
    "            BasicLayerConfig(\n",
    "                embedding=64,\n",
    "                attention_mechanism=\"pooling\",\n",
    "                patch_size=3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                seq_len=image_size * image_size // 4,\n",
    "                feedforward=\"Conv2DFeedforward\",\n",
    "            ),\n",
    "            BasicLayerConfig(\n",
    "                embedding=128,\n",
    "                attention_mechanism=\"pooling\",\n",
    "                patch_size=3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                seq_len=image_size * image_size // 16,\n",
    "                feedforward=\"Conv2DFeedforward\",\n",
    "            ),\n",
    "            BasicLayerConfig(\n",
    "                embedding=320,\n",
    "                attention_mechanism=\"pooling\",\n",
    "                patch_size=3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                seq_len=image_size * image_size // 64,\n",
    "                feedforward=\"Conv2DFeedforward\",\n",
    "            ),\n",
    "            # L1 would have an extra layer here, similar to the above,\n",
    "            # bringing the sequence length down to HxW / 1024\n",
    "        ]\n",
    "\n",
    "        # Fill in the gaps in the config\n",
    "        xformer_config = get_hierarchical_configuration(\n",
    "            base_hierarchical_configs,\n",
    "            layernorm_style=\"pre\",\n",
    "            use_rotary_embeddings=False,\n",
    "            mlp_multiplier=4,\n",
    "            in_channels=3,  # 24 if L1, there's another stem prior to the trunk\n",
    "        )\n",
    "\n",
    "        # Now instantiate the EfficientFormer trunk\n",
    "        config = xFormerConfig(xformer_config)\n",
    "        config.weight_init = \"moco\"\n",
    "\n",
    "        self.trunk = xFormer.from_config(config)\n",
    "\n",
    "        # L1 model\n",
    "        # # This model requires a pre-stem (a conv prior to going through all the layers above)\n",
    "        # self.pre_stem = build_patch_embedding(\n",
    "        #     PatchEmbeddingConfig(\n",
    "        #         in_channels=3, out_channels=24, kernel_size=3, stride=2, padding=1\n",
    "        #     )\n",
    "        # )\n",
    "\n",
    "        # This model requires a final Attention step\n",
    "        self.attention = MultiHeadDispatch(\n",
    "            dim_model=320, num_heads=4, attention=ScaledDotProduct()\n",
    "        )\n",
    "\n",
    "        # The classifier head\n",
    "        dim = base_hierarchical_configs[-1].embedding\n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "        self.head = nn.Linear(dim, num_classes)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.val_accuracy = Accuracy()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(-2, -1).transpose(-1, -2)  # BCHW to BSE\n",
    "        # x = self.pre_stem(x) # L1 model\n",
    "        x = self.trunk(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.ln(x)\n",
    "\n",
    "        x = x.mean(dim=1)  # mean over sequence len\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we need a training script, let's try this out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "/home/lefaudeux/.conda/envs/xformers/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:184: LightningDeprecationWarning: DataModule property `size` was deprecated in v1.5 and will be removed in v1.7.\n",
      "  rank_zero_deprecation(\"DataModule property `size` was deprecated in v1.5 and will be removed in v1.7.\")\n",
      "WARNING:root:Not initializing weights in 1, this could be a mistake.\n",
      "Module Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "WARNING:root:Not initializing weights in 1, this could be a mistake.\n",
      "Module Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "WARNING:root:Not initializing weights in 1, this could be a mistake.\n",
      "Module Conv2d(128, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'block_type': 'encoder', 'dim_model': 64, 'use_triton': False, 'layer_norm_style': 'pre', 'multi_head_config': {'num_heads': 1, 'use_rotary_embeddings': False, 'attention': {'name': 'pooling'}}, 'feedforward_config': {'name': 'Conv2DFeedforward', 'activation': 'gelu', 'hidden_layer_multiplier': 4, 'dropout': 0.0}, 'position_encoding_config': {'name': 'learnable', 'seq_len': 256, 'add_class_token': False}, 'patch_embedding_config': {'in_channels': 3, 'kernel_size': 3, 'stride': 2, 'padding': 1}}\n",
      "{'block_type': 'encoder', 'dim_model': 128, 'use_triton': False, 'layer_norm_style': 'pre', 'multi_head_config': {'num_heads': 1, 'use_rotary_embeddings': False, 'attention': {'name': 'pooling'}}, 'feedforward_config': {'name': 'Conv2DFeedforward', 'activation': 'gelu', 'hidden_layer_multiplier': 4, 'dropout': 0.0}, 'position_encoding_config': {'name': 'learnable', 'seq_len': 64, 'add_class_token': False}, 'patch_embedding_config': {'in_channels': 64, 'kernel_size': 3, 'stride': 2, 'padding': 1}}\n",
      "{'block_type': 'encoder', 'dim_model': 320, 'use_triton': False, 'layer_norm_style': 'pre', 'multi_head_config': {'num_heads': 1, 'use_rotary_embeddings': False, 'attention': {'name': 'pooling'}}, 'feedforward_config': {'name': 'Conv2DFeedforward', 'activation': 'gelu', 'hidden_layer_multiplier': 4, 'dropout': 0.0}, 'position_encoding_config': {'name': 'learnable', 'seq_len': 16, 'add_class_token': False}, 'patch_embedding_config': {'in_channels': 128, 'kernel_size': 3, 'stride': 2, 'padding': 1}}\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "You requested GPUs: [0]\n But your machine only has: []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m/home/lefaudeux/Git/xformers/docs/source/xformers_cifar_efficient_former.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lefaudeux/Git/xformers/docs/source/xformers_cifar_efficient_former.ipynb#ch0000006?line=29'>30</a>\u001b[0m steps \u001b[39m=\u001b[39m dm\u001b[39m.\u001b[39mnum_samples \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m REF_BATCH \u001b[39m*\u001b[39m MAX_EPOCHS\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lefaudeux/Git/xformers/docs/source/xformers_cifar_efficient_former.ipynb#ch0000006?line=30'>31</a>\u001b[0m lm \u001b[39m=\u001b[39m EfficientFormer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lefaudeux/Git/xformers/docs/source/xformers_cifar_efficient_former.ipynb#ch0000006?line=31'>32</a>\u001b[0m     steps\u001b[39m=\u001b[39msteps,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lefaudeux/Git/xformers/docs/source/xformers_cifar_efficient_former.ipynb#ch0000006?line=32'>33</a>\u001b[0m     image_size\u001b[39m=\u001b[39mimage_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lefaudeux/Git/xformers/docs/source/xformers_cifar_efficient_former.ipynb#ch0000006?line=33'>34</a>\u001b[0m     num_classes\u001b[39m=\u001b[39mnum_classes,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lefaudeux/Git/xformers/docs/source/xformers_cifar_efficient_former.ipynb#ch0000006?line=34'>35</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/lefaudeux/Git/xformers/docs/source/xformers_cifar_efficient_former.ipynb#ch0000006?line=35'>36</a>\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39;49mTrainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lefaudeux/Git/xformers/docs/source/xformers_cifar_efficient_former.ipynb#ch0000006?line=36'>37</a>\u001b[0m     gpus\u001b[39m=\u001b[39;49mGPUS,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lefaudeux/Git/xformers/docs/source/xformers_cifar_efficient_former.ipynb#ch0000006?line=37'>38</a>\u001b[0m     max_epochs\u001b[39m=\u001b[39;49mMAX_EPOCHS,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lefaudeux/Git/xformers/docs/source/xformers_cifar_efficient_former.ipynb#ch0000006?line=38'>39</a>\u001b[0m     precision\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lefaudeux/Git/xformers/docs/source/xformers_cifar_efficient_former.ipynb#ch0000006?line=39'>40</a>\u001b[0m     accumulate_grad_batches\u001b[39m=\u001b[39;49mREF_BATCH \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m BATCH,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lefaudeux/Git/xformers/docs/source/xformers_cifar_efficient_former.ipynb#ch0000006?line=40'>41</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lefaudeux/Git/xformers/docs/source/xformers_cifar_efficient_former.ipynb#ch0000006?line=41'>42</a>\u001b[0m trainer\u001b[39m.\u001b[39mfit(lm, dm)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lefaudeux/Git/xformers/docs/source/xformers_cifar_efficient_former.ipynb#ch0000006?line=43'>44</a>\u001b[0m \u001b[39m# check the training\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/xformers/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/env_vars_connector.py:38\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mlist\u001b[39m(env_variables\u001b[39m.\u001b[39mitems()) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(kwargs\u001b[39m.\u001b[39mitems()))\n\u001b[1;32m     37\u001b[0m \u001b[39m# all args were already moved to kwargs\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/xformers/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:426\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, logger, checkpoint_callback, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, process_position, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, log_gpu_memory, progress_bar_refresh_rate, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, flush_logs_every_n_steps, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, weights_summary, weights_save_path, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, reload_dataloaders_every_epoch, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, prepare_data_per_node, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode, stochastic_weight_avg, terminate_on_nan)\u001b[0m\n\u001b[1;32m    423\u001b[0m Trainer\u001b[39m.\u001b[39m_log_api_event(\u001b[39m\"\u001b[39m\u001b[39minit\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    424\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m TrainerState()\n\u001b[0;32m--> 426\u001b[0m gpu_ids, tpu_cores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_devices(gpus, auto_select_gpus, tpu_cores)\n\u001b[1;32m    428\u001b[0m \u001b[39m# init connectors\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector \u001b[39m=\u001b[39m DataConnector(\u001b[39mself\u001b[39m, multiple_trainloader_mode)\n",
      "File \u001b[0;32m~/.conda/envs/xformers/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1543\u001b[0m, in \u001b[0;36mTrainer._parse_devices\u001b[0;34m(gpus, auto_select_gpus, tpu_cores)\u001b[0m\n\u001b[1;32m   1540\u001b[0m     gpus \u001b[39m=\u001b[39m pick_multiple_gpus(gpus)\n\u001b[1;32m   1542\u001b[0m \u001b[39m# TODO (@seannaren, @kaushikb11): Include IPU parsing logic here\u001b[39;00m\n\u001b[0;32m-> 1543\u001b[0m gpu_ids \u001b[39m=\u001b[39m device_parser\u001b[39m.\u001b[39;49mparse_gpu_ids(gpus)\n\u001b[1;32m   1544\u001b[0m tpu_cores \u001b[39m=\u001b[39m device_parser\u001b[39m.\u001b[39mparse_tpu_cores(tpu_cores)\n\u001b[1;32m   1545\u001b[0m \u001b[39mreturn\u001b[39;00m gpu_ids, tpu_cores\n",
      "File \u001b[0;32m~/.conda/envs/xformers/lib/python3.8/site-packages/pytorch_lightning/utilities/device_parser.py:89\u001b[0m, in \u001b[0;36mparse_gpu_ids\u001b[0;34m(gpus)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39m# Check that gpus are unique. Duplicate gpus are not supported by the backend.\u001b[39;00m\n\u001b[1;32m     87\u001b[0m _check_unique(gpus)\n\u001b[0;32m---> 89\u001b[0m \u001b[39mreturn\u001b[39;00m _sanitize_gpu_ids(gpus)\n",
      "File \u001b[0;32m~/.conda/envs/xformers/lib/python3.8/site-packages/pytorch_lightning/utilities/device_parser.py:151\u001b[0m, in \u001b[0;36m_sanitize_gpu_ids\u001b[0;34m(gpus)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39mfor\u001b[39;00m gpu \u001b[39min\u001b[39;00m gpus:\n\u001b[1;32m    150\u001b[0m     \u001b[39mif\u001b[39;00m gpu \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m all_available_gpus:\n\u001b[0;32m--> 151\u001b[0m         \u001b[39mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    152\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou requested GPUs: \u001b[39m\u001b[39m{\u001b[39;00mgpus\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m But your machine only has: \u001b[39m\u001b[39m{\u001b[39;00mall_available_gpus\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         )\n\u001b[1;32m    154\u001b[0m \u001b[39mreturn\u001b[39;00m gpus\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: You requested GPUs: [0]\n But your machine only has: []"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(42)\n",
    "\n",
    "# Adjust batch depending on the available memory on your machine.\n",
    "# You can also use reversible layers to save memory\n",
    "REF_BATCH = 768\n",
    "BATCH = 768  # lower if not enough GPU memory\n",
    "\n",
    "MAX_EPOCHS = 50\n",
    "NUM_WORKERS = 4\n",
    "GPUS = 1\n",
    "\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# We'll use a datamodule here, which already handles dataset/dataloader/sampler\n",
    "# See https://pytorchlightning.github.io/lightning-tutorials/notebooks/lightning_examples/cifar10-baseline.html\n",
    "# for a full tutorial\n",
    "dm = CIFAR10DataModule(\n",
    "    data_dir=\"data\",\n",
    "    batch_size=BATCH,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "image_size = dm.size(-1)  # 32 for CIFAR\n",
    "num_classes = dm.num_classes  # 10 for CIFAR\n",
    "\n",
    "# compute total number of steps\n",
    "batch_size = BATCH * GPUS\n",
    "steps = dm.num_samples // REF_BATCH * MAX_EPOCHS\n",
    "lm = EfficientFormer(\n",
    "    steps=steps,\n",
    "    image_size=image_size,\n",
    "    num_classes=num_classes,\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    gpus=GPUS,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    precision=16,\n",
    "    accumulate_grad_batches=REF_BATCH // BATCH,\n",
    ")\n",
    "trainer.fit(lm, dm)\n",
    "\n",
    "# check the training\n",
    "trainer.test(lm, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('xformers')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d3fe8a1207b65a47d3f6629836261882a45fc1ee1d9018464faac0a6e60e889"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
