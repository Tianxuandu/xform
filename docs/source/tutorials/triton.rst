Using Triton-based layers
=========================

Triton_ is a language and compiler for parallel programming, currently applicable to CUDA-enabled GPUs.
It is compatible with PyTorch CUDA Tensors, and can be interfaced directly with pure python code.


PyTorch provides many primitives capable of tranforming tensors, which correspond to operators in each of the supported backends.
There are limits to how many of them can be supported at any point in time, short of supporting a JIT toolchain,
so some operations typical of the Transformer family are supported in PyTorch as a sequence of base operators.

Triton makes it possible to consolidate some of them into ad-hoc fused operators, which are compiled just-in-time.
xFormers proposes a couple of optimized layers, and the goal is to increase their number over time.


Fused softmax layer
-------------------

This is a drop-in replacement to `torch.nn.softmax`_, the only limitation being that the softmax operation is limited to the last dimension.
Log-softmax is also available. The actual Triton kernel is very similar to `this tutorial<https://triton-lang.org/getting-started/tutorials/02-fused-softmax.html#sphx-glr-getting-started-tutorials-02-fused-softmax-py>`

.. code-block:: python

    from xformers.triton import softmax, log_softmax
    y = softmax(x)   # Torch AMP, autograd aware

The expected throughput, when compared to PyTorch and on a nVidia V100, is along these lines

+-----------------------------------+----------------------+--------------------+--------------------+----------------------+--------------------+---------------------+--------------------------+
| torch.float16                     | Unit: GB/s           |                    |                    |                      |                    |                     |                          |
+===================================+======================+====================+====================+======================+====================+=====================+==========================+
|                                   |    B=8, M=384, K=128 |  B=8, M=784, K=512 |  B=4, M=2048, K=384|  B=4, M=3136, K=1024 | B=2, M=1024, K=2048| B=2, M=2048, K=4096 | B=2, M=4096, K=4096      |
+-----------------------------------+----------------------+--------------------+--------------------+----------------------+--------------------+---------------------+--------------------------+
|pytorch - fw                       |   170.7              | 501.8              | 512.0              | 597.3                | 399.6              | 524.3               | 553.0                    |
+-----------------------------------+----------------------+--------------------+--------------------+----------------------+--------------------+---------------------+--------------------------+
|triton  - fw                       |   153.6              | 522.7              | 512.0              | 716.8                | 606.8              | 736.4               | 775.6                    |
+-----------------------------------+----------------------+--------------------+--------------------+----------------------+--------------------+---------------------+--------------------------+
|pytorch - log - fw                 |   192.0              | 545.4              | 534.3              | 669.0                | 496.5              | 601.2               | 615.4                    |
+-----------------------------------+----------------------+--------------------+--------------------+----------------------+--------------------+---------------------+--------------------------+
|triton  - log - fw                 |   153.6              | 570.2              | 558.5              | 748.9                | 682.7              | 780.2               | 799.2                    |
+-----------------------------------+----------------------+--------------------+--------------------+----------------------+--------------------+---------------------+--------------------------+
|pytorch - fw+bw                    |   71.4               | 170.7              | 168.3              | 205.6                | 164.7              | 196.5               | 203.5                    |
+-----------------------------------+----------------------+--------------------+--------------------+----------------------+--------------------+---------------------+--------------------------+
|triton  - fw+bw                    |   69.8               | 218.2              | 211.9              | 264.8                | 224.4              | 271.4               | 284.3                    |
+-----------------------------------+----------------------+--------------------+--------------------+----------------------+--------------------+---------------------+--------------------------+
|pytorch - log - fw+bw              |   78.8               | 207.3              | 204.8              | 255.3                | 206.1              | 247.3               | 255.5                    |
+-----------------------------------+----------------------+--------------------+--------------------+----------------------+--------------------+---------------------+--------------------------+
|triton  - log - fw+bw              |   71.4               | 220.1              | 213.7              | 266.9                | 229.1              | 273.6               | 285.6                    |
+-----------------------------------+----------------------+--------------------+--------------------+----------------------+--------------------+---------------------+--------------------------+


+-----------------------------------+----------------------+--------------------+--------------------+----------------------+--------------------+---------------------+--------------------------+
| torch.float32                     | Unit: GB/s           |                    |                    |                      |                    |                     |                          |
+===================================+======================+====================+====================+======================+====================+=====================+==========================+
|                                   |   B=8, M=384, K=128  | B=8, M=784, K=512  | B=4, M=2048, K=384 | B=4, M=3136, K=1024  | B=2, M=1024, K=2048| B=2, M=2048, K=4096 | B=2, M=4096, K=4096      |
+-----------------------------------+----------------------+--------------------+--------------------+----------------------+--------------------+---------------------+--------------------------+
|pytorch - fw                       |   341.3              | 660.2              | 682.7              | 760.2                | 555.4              | 636.3               | 650.5                    |
+-----------------------------------+----------------------+--------------------+--------------------+----------------------+--------------------+---------------------+--------------------------+
|triton  - fw                       |   307.2              | 678.1              | 682.7              | 784.0                | 712.3              | 789.6               | 809.1                    |
+-----------------------------------+----------------------+--------------------+--------------------+----------------------+--------------------+---------------------+--------------------------+
|pytorch - log - fw                 |   384.0              | 696.9              | 702.2              | 777.9                | 537.2              | 541.6               | 543.9                    |
+-----------------------------------+----------------------+--------------------+--------------------+----------------------+--------------------+---------------------+--------------------------+
|triton  - log - fw                 |   307.2              | 696.9              | 702.2              | 796.4                | 744.7              | 799.2               | 814.1                    |
+-----------------------------------+----------------------+--------------------+--------------------+----------------------+--------------------+---------------------+--------------------------+
|pytorch - fw+bw                    |   133.6              | 203.1              | 204.0              | 229.9                | 193.9              | 211.1               | 215.3                    |
+-----------------------------------+----------------------+--------------------+--------------------+----------------------+--------------------+---------------------+--------------------------+
|triton  - fw+bw                    |   136.5              | 254.7              | 257.3              | 290.9                | 263.2              | 294.5               | 301.0                    |
+-----------------------------------+----------------------+--------------------+--------------------+----------------------+--------------------+---------------------+--------------------------+
|pytorch - log - fw+bw              |   149.9              | 252.1              | 252.1              | 289.6                | 234.1              | 251.6               | 254.5                    |
+-----------------------------------+----------------------+--------------------+--------------------+----------------------+--------------------+---------------------+--------------------------+
|triton  - log - fw+bw              |   136.5              | 257.3              | 258.7              | 291.7                | 265.3              | 295.2               | 301.3                    |
+-----------------------------------+----------------------+--------------------+--------------------+----------------------+--------------------+---------------------+--------------------------+


Fused linear layer
-------------------
This is a drop-in replacement to two PyTorch operands: a `torch.nn.Linear`, and an activation, like `torch.nn.ReLU`. It is Torch AMP and autograd aware, and can be used very simply:

.. code-block:: python

    from xformers.triton import FusedLinearLayer

    my_linear_layer = FusedLinearLayer(in_features, out_features, bias=True/False, activation="squared_relu")

    ...

    y = my_linear_layer(x)

It is possible to skip either the bias or the activation (just use `None` in that case). As of September 2021, this layer is **faster than PyTorch for non-sigmoid activations and fp16**.
In all other usecases, you will be better served using PyTorch.

The following is an example of the measured performance on a nVidia V100.

+-----------------------------------------+----------------------+--------------------+--------------------+----------------------+--------------------+
| torch.float16                           | Unit: TFlops         |                    |                    |                      |                    |
+=========================================+======================+====================+====================+======================+====================+
|                                         | B=8, M=256, K=512    | B=8, M=512, K=1024 | B=4, M=1024, K=1024| B=2, M=2048, K=2048  | B=2, M=4096, K=4096|
+-----------------------------------------+----------------------+--------------------+--------------------+----------------------+--------------------+
| pytorch - squared_relu -  bias - fw     |     6.3              |    12.4            |     12.3           |      17.1            |     19.0           |
+-----------------------------------------+----------------------+--------------------+--------------------+----------------------+--------------------+
| triton  - squared_relu -  bias - fw     |     13.8             |    18.9            |     18.9           |      21.9            |     21.7           |
+-----------------------------------------+----------------------+--------------------+--------------------+----------------------+--------------------+
| pytorch - squared_relu -  bias - fw+bw  |     4.0              |    7.6             |     7.7            |      10.7            |     12.6           |
+-----------------------------------------+----------------------+--------------------+--------------------+----------------------+--------------------+
| triton  - squared_relu -  bias - fw+bw  |     8.4              |    13.5            |     13.3           |      15.9            |     16.8           |
+-----------------------------------------+----------------------+--------------------+--------------------+----------------------+--------------------+



.. _Triton: https://triton-lang.org/
.. _`torch.nn.softmax`: https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html
